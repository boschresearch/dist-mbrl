"""
Copyright (c) 2024 Robert Bosch GmbH

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

from typing import Dict, List, Optional, Union, cast

import numpy as np

from dist_mbrl.config import AgentTypes
from dist_mbrl.mbrl.models import BasicEnsemble, ModelEnv
from dist_mbrl.mbrl.planning import Agent
from dist_mbrl.mbrl.types import TransitionBatch
from dist_mbrl.mbrl.util import ReplayBuffer

ModelBufferType = Union[ReplayBuffer, List[ReplayBuffer]]


def collect_ensemble_model_transitions(
    ensemble_envs: List[ModelEnv],
    agent: Agent,
    replay_buffer: ModelBufferType,
    env_replay_buffer: ReplayBuffer,
    rollout_length: int,
    batch_size: int,
    rollout_mode: Optional[str] = "random_model",
):
    """
    Collects model-generated data and stores it in replay buffer(s), depending
    on what algorithm we are executing.

    MBPO / QRMBPO --> Single replay buffer filled from single ModelEnv
    EQRSAC --> Single replay buffer filled from multiple ModelEnv
    QUSAC --> Multiple replay buffers filled from multiple ModelEnv
    """
    init_state_dist = env_replay_buffer

    if agent.type == AgentTypes.QUSAC:
        for env, buffer in zip(ensemble_envs[::-1], replay_buffer[::-1]):
            if isinstance(env.dynamics_model.model, BasicEnsemble):
                env.dynamics_model.set_propagation_method(rollout_mode)
            collect_single_model_transitions(
                env,
                agent,
                buffer,
                rollout_length,
                batch_size,
                init_state_dist=init_state_dist,
            )
            env.dynamics_model.set_propagation_method()
    else:
        for env in ensemble_envs:
            # For MBPO approaches we only use the random model propagation method for
            # data collection
            if agent.type in (
                AgentTypes.MBPO,
                AgentTypes.QRMBPO,
                AgentTypes.QRMBPO_BIG,
                AgentTypes.QRMBPO_LOSS,
            ):
                env.dynamics_model.set_propagation_method(rollout_mode)
                if agent.type == AgentTypes.QRMBPO_BIG:
                    batch_size *= len(env.dynamics_model)
            collect_single_model_transitions(
                env,
                agent,
                replay_buffer,
                rollout_length,
                batch_size,
                init_state_dist=init_state_dist,
            )
            # Always reset the model to return the predictions from all ensemble members
            env.dynamics_model.set_propagation_method()


def collect_single_model_transitions(
    model_env: ModelEnv,
    agent: Agent,
    replay_buffer: ReplayBuffer,
    rollout_horizon: int,
    batch_size: int,
    agent_kwargs: Dict = {},
    init_state_dist: Optional[ReplayBuffer] = None,
):
    """
    Collects data generated by rollouts under a transition model, which has been wrapped
    around a gym-like environment. Notably, this method does not keep track of any
    trajectory information, thus must be paired exclusively with a
    `basic_buffer_iterator`, see train.ube.py for example.
    """
    if init_state_dist is not None:
        batch = init_state_dist.sample(batch_size)
        initial_obs, act, next_obs, rewards, *_ = cast(TransitionBatch, batch).astuple()
    else:
        initial_obs = np.array(
            [model_env.observation_space.sample() for _ in range(batch_size)]
        )

    model_state = model_env.reset(
        initial_obs_batch=cast(np.ndarray, initial_obs),
        return_as_np=True,
    )
    accum_dones = np.zeros(initial_obs.shape[0], dtype=bool)
    obs = initial_obs
    for _ in range(rollout_horizon):
        action = agent.act(obs, **agent_kwargs)
        try:
            pred_next_obs, pred_rewards, pred_dones, model_state = model_env.step(
                action, model_state, sample=True
            )
        except RuntimeError as err:
            print(
                f"Terminating model rollout due to error during dynamics model forward pass: {err}"
            )
            break

        replay_buffer.add_batch(
            obs[~accum_dones],
            action[~accum_dones],
            pred_next_obs[~accum_dones],
            pred_rewards[~accum_dones, 0],
            pred_dones[~accum_dones, 0],
        )
        obs = pred_next_obs
        accum_dones |= pred_dones.squeeze()
