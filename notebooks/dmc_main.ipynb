{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "root_module = Path.cwd()\n",
    "ext = \".pkl\"\n",
    "file_name = \"dmc_main\"\n",
    "load_dir = root_module.parent.joinpath(\"data/\")\n",
    "file_dir = load_dir.joinpath(file_name + ext)\n",
    "raw_data = pickle.load(open(file_dir, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_mbrl.utils.process_results import (\n",
    "    get_bootstrap_intervals,\n",
    "    get_performance_profile,\n",
    "    process_raw_results,\n",
    ")\n",
    "\n",
    "# Post-process raw return data to get mean and standard error\n",
    "metric_data = process_raw_results(raw_data)\n",
    "\n",
    "# Aggregate metric_data across environments for global statistics\n",
    "aggregated_metrics = get_bootstrap_intervals(metric_data)\n",
    "\n",
    "\n",
    "# Retrieve relevant variables from data\n",
    "env_names = metric_data[\"env_names\"]\n",
    "steps = metric_data[\"steps\"]\n",
    "mean_returns = metric_data[\"mean_returns\"]\n",
    "ci_returns = metric_data[\"ci_returns\"]\n",
    "\n",
    "# Aggregated metrics\n",
    "agg_mean_returns = aggregated_metrics[\"mean_returns\"]\n",
    "agg_median_returns = aggregated_metrics[\"median_returns\"]\n",
    "agg_iqm_returns = aggregated_metrics[\"iqm_returns\"]\n",
    "agg_mean_ci_returns = aggregated_metrics[\"mean_ci_returns\"]\n",
    "agg_median_ci_returns = aggregated_metrics[\"median_ci_returns\"]\n",
    "agg_iqm_ci_returns = aggregated_metrics[\"iqm_ci_returns\"]\n",
    "agg_steps = aggregated_metrics[\"steps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rliable` performance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_profile = get_performance_profile(metric_data, episode=-1)\n",
    "\n",
    "perf_score_dist = performance_profile[\"score_distributions\"]\n",
    "perf_score_ci_dist = performance_profile[\"score_cis\"]\n",
    "perf_steps = performance_profile[\"steps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print scores to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = root_module.parent.joinpath(\"data/dmc_scores.txt\")\n",
    "file = open(file_dir, \"w\")\n",
    "\n",
    "\n",
    "def score_label(params):\n",
    "    label = \"\"\n",
    "    for param in params[1:]:\n",
    "        if str(param) != \"nan\":\n",
    "            label += f\"{param} \"\n",
    "    return label\n",
    "\n",
    "\n",
    "for env_name in env_names:\n",
    "    file.write(f\"{env_name}\\n\")\n",
    "    for idx in mean_returns[env_name].keys():\n",
    "        file.write(\n",
    "            f\"\\t{score_label(idx)}: {mean_returns[env_name][idx][-1]:.1f} +/- {ci_returns[env_name][idx][-1]:.1f}\\n\"\n",
    "        )\n",
    "\n",
    "file.write(\"Mean\\n\")\n",
    "for idx in mean_returns[env_names[0]].keys():\n",
    "    file.write(\n",
    "        f\"\\t{score_label(idx)}(100): {agg_mean_returns[idx][99]:.2f} ({agg_mean_ci_returns[idx][0][99]:.2f}, {agg_mean_ci_returns[idx][1][99]:.2f})\\n\"\n",
    "    )\n",
    "    file.write(\n",
    "        f\"\\t{score_label(idx)}(250): {agg_mean_returns[idx][-1]:.2f} ({agg_mean_ci_returns[idx][0][-1]:.2f}, {agg_mean_ci_returns[idx][1][-1]:.2f})\\n\"\n",
    "    )\n",
    "\n",
    "file.write(\"Median\\n\")\n",
    "for idx in mean_returns[env_names[0]].keys():\n",
    "    file.write(\n",
    "        f\"\\t{score_label(idx)}(100): {agg_median_returns[idx][99]:.2f} ({agg_median_ci_returns[idx][0][99]:.2f}, {agg_median_ci_returns[idx][1][99]:.2f})\\n\"\n",
    "    )\n",
    "    file.write(\n",
    "        f\"\\t{score_label(idx)}(250): {agg_median_returns[idx][-1]:.2f} ({agg_median_ci_returns[idx][0][-1]:.2f}, {agg_median_ci_returns[idx][1][-1]:.2f})\\n\"\n",
    "    )\n",
    "\n",
    "file.write(\"IQM\\n\")\n",
    "for idx in mean_returns[env_names[0]].keys():\n",
    "    file.write(\n",
    "        f\"\\t{score_label(idx)}(100): {agg_iqm_returns[idx][99]:.2f} ({agg_iqm_ci_returns[idx][0][99]:.2f}, {agg_iqm_ci_returns[idx][1][99]:.2f})\\n\"\n",
    "    )\n",
    "    file.write(\n",
    "        f\"\\t{score_label(idx)}(250): {agg_iqm_returns[idx][-1]:.2f} ({agg_iqm_ci_returns[idx][0][-1]:.2f}, {agg_iqm_ci_returns[idx][1][-1]:.2f})\\n\"\n",
    "    )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting - Full benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from dist_mbrl.utils.plot import (\n",
    "    JMLR_PARAMS,\n",
    "    LIGHT_GREY,\n",
    "    default_process_label,\n",
    "    handle_2D_axes_and_legend,\n",
    "    plot_with_intervals,\n",
    "    plot_with_symmetric_intervals,\n",
    ")\n",
    "\n",
    "plt.rcParams.update(JMLR_PARAMS)\n",
    "\n",
    "# Define grid of plots\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(len(env_names) / ncols))\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(6.5, 6.0),\n",
    "    gridspec_kw={\"wspace\": 0.45, \"hspace\": 0.50},\n",
    ")\n",
    "\n",
    "# Reference axes by environment name\n",
    "ax_dict = {}\n",
    "for idx, ax in zip(env_names, axes.flatten()):\n",
    "    ax_dict[idx] = ax\n",
    "\n",
    "# Assign colors to each of the methods we are comparing\n",
    "colors = {}\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "for i, type in enumerate(mean_returns[env_names[0]].keys()):\n",
    "    colors[type] = cmap(i)\n",
    "\n",
    "# Plot for all environments and all methods\n",
    "ep_length = 1000\n",
    "for env_name in env_names:\n",
    "    for idx in mean_returns[env_name].keys():\n",
    "        plot_with_symmetric_intervals(\n",
    "            ax=ax_dict[env_name],\n",
    "            x=steps[env_name][idx] // ep_length,\n",
    "            y=mean_returns[env_name][idx],\n",
    "            yerr=ci_returns[env_name][idx],\n",
    "            label=default_process_label(idx[1:]),\n",
    "            title=env_name,\n",
    "            color=colors[idx],\n",
    "        )\n",
    "\n",
    "handle_2D_axes_and_legend(axes=axes, legend_ncol=4, legend_offset=(2.6, -5.6))\n",
    "\n",
    "# Locally modify the title of cartpole envs since they are too long\n",
    "axes[0, 2].set_title(\"cartpole-balance\")\n",
    "axes[0, 3].set_title(\"cartpole-swingup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQM Return and Performance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid of plots\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(len(env_names) / ncols))\n",
    "fig_agg, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=ncols,\n",
    "    figsize=(6.5, 2.0),\n",
    "    gridspec_kw={\"wspace\": 0.4, \"hspace\": 0.50},\n",
    ")\n",
    "\n",
    "# Assign colors to each of the methods we are comparing\n",
    "colors = {}\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "for i, type in enumerate(mean_returns[env_names[0]].keys()):\n",
    "    colors[type] = cmap(i)\n",
    "\n",
    "# IQM Plot\n",
    "for idx in mean_returns[env_names[0]].keys():\n",
    "    plot_with_intervals(\n",
    "        ax=axes[0],\n",
    "        x=agg_steps[:250] // ep_length,\n",
    "        y=agg_iqm_returns[idx],\n",
    "        ylow=agg_iqm_ci_returns[idx][0],\n",
    "        yhigh=agg_iqm_ci_returns[idx][1],\n",
    "        label=default_process_label(idx[1:]),\n",
    "        color=colors[idx],\n",
    "        grid_color=LIGHT_GREY,\n",
    "    )\n",
    "\n",
    "axes[0].set_ylabel(\"Normalized IQM Return\")\n",
    "axes[0].set_xlabel(\"Episodes\")\n",
    "\n",
    "# Performance Profile\n",
    "for idx in mean_returns[env_names[0]].keys():\n",
    "    plot_with_intervals(\n",
    "        ax=axes[1],\n",
    "        x=perf_steps,\n",
    "        y=perf_score_dist[idx],\n",
    "        ylow=perf_score_ci_dist[idx][0],\n",
    "        yhigh=perf_score_ci_dist[idx][1],\n",
    "        label=default_process_label(idx[1:]),\n",
    "        color=colors[idx],\n",
    "        grid_color=LIGHT_GREY,\n",
    "    )\n",
    "\n",
    "axes[1].set_ylabel(\"Fraction of runs\" + \"\\n\" + r\"with score $>\\tau$\")\n",
    "axes[1].set_xlabel(r\"Normalized score ($\\tau$)\")\n",
    "\n",
    "axes[0].legend(loc=\"lower center\", ncol=4, bbox_to_anchor=(1.1, -0.65), frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom of performance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_zoom, ax = plt.subplots(\n",
    "    nrows=1, ncols=1, figsize=(2.5, 1.0), gridspec_kw={\"wspace\": 0.4, \"hspace\": 0.50}\n",
    ")\n",
    "for idx in mean_returns[env_names[0]].keys():\n",
    "    plot_with_intervals(\n",
    "        ax=ax,\n",
    "        x=perf_steps[150:250],\n",
    "        y=perf_score_dist[idx][150:250],\n",
    "        ylow=perf_score_ci_dist[idx][0][150:250],\n",
    "        yhigh=perf_score_ci_dist[idx][1][150:250],\n",
    "        label=default_process_label(idx[1:]),\n",
    "        color=colors[idx],\n",
    "        grid_color=LIGHT_GREY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting - Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_mbrl.utils.plot import handle_1D_axes_and_legend\n",
    "\n",
    "envs_to_plot = [\n",
    "    \"cartpole-swingup_sparse\",\n",
    "    \"cheetah-run\",\n",
    "    \"quadruped-run\",\n",
    "    \"walker-run\",\n",
    "]\n",
    "\n",
    "# Define grid of plots\n",
    "ncols = 4\n",
    "nrows = 1\n",
    "fig_lite, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(6.8, 1.0),\n",
    "    gridspec_kw={\"wspace\": 0.45, \"hspace\": 0.50},\n",
    ")\n",
    "\n",
    "# Reference axes by environment name\n",
    "ax_dict = {}\n",
    "for idx, ax in zip(envs_to_plot, axes.flatten()):\n",
    "    ax_dict[idx] = ax\n",
    "\n",
    "for env_name in envs_to_plot:\n",
    "    for idx in mean_returns[env_name].keys():\n",
    "        plot_with_symmetric_intervals(\n",
    "            ax=ax_dict[env_name],\n",
    "            x=steps[env_name][idx] // ep_length,\n",
    "            y=mean_returns[env_name][idx],\n",
    "            yerr=ci_returns[env_name][idx],\n",
    "            label=default_process_label(idx[1:]),\n",
    "            title=env_name,\n",
    "            color=colors[idx],\n",
    "        )\n",
    "\n",
    "handle_1D_axes_and_legend(axes=axes, legend_ncol=4, legend_offset=(2.6, -1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = root_module.parent.joinpath(\"figures/dmc_main_appendix.pdf\")\n",
    "fig.savefig(fig_dir, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "fig_dir = root_module.parent.joinpath(\"figures/dmc_rliable.pdf\")\n",
    "fig_agg.savefig(fig_dir, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "fig_dir = root_module.parent.joinpath(\"figures/dmc_perf_profile_zoom.pdf\")\n",
    "fig_zoom.savefig(fig_dir, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "fig_dir = root_module.parent.joinpath(\"figures/dmc_main.pdf\")\n",
    "fig_lite.savefig(fig_dir, bbox_inches=\"tight\", transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    ">Copyright (c) 2024 Robert Bosch GmbH\n",
    ">\n",
    ">This program is free software: you can redistribute it and/or modify <br>\n",
    ">it under the terms of the GNU Affero General Public License as published<br>\n",
    ">by the Free Software Foundation, either version 3 of the License, or<br>\n",
    ">(at your option) any later version.<br>\n",
    ">\n",
    ">This program is distributed in the hope that it will be useful,<br>\n",
    ">but WITHOUT ANY WARRANTY; without even the implied warranty of<br>\n",
    ">MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br>\n",
    ">GNU Affero General Public License for more details.<br>\n",
    ">\n",
    ">You should have received a copy of the GNU Affero General Public License<br>\n",
    ">along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d44ccd9663ca780629f4890d49070f732c0f5e9dadc4d8a50f711cbdaf1c170b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('dist_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
